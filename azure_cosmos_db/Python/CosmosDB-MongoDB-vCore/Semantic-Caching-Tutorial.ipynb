{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e01527",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we'll demonstrate how to leverage a semantic caching with Azure Cosmos DB for MongoDB and LangChain.\n",
    "\n",
    "[Learn more here from the LangChain docs.](https://python.langchain.com/docs/integrations/llms/llm_caching#azure-cosmos-db-semantic-cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install langchain_openai\n",
    "# ! pip install langchain_community\n",
    "# ! pip install pymongo\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install azure-core\n",
    "# ! pip install azure-cosmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9473250-4507-40d0-be60-5277f40200f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cache import AzureCosmosDBSemanticCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import urllib \n",
    "\n",
    "AzureCosmosDBSemanticCache\n",
    "from langchain_community.vectorstores.azure_cosmos_db import (\n",
    "    CosmosDBSimilarityType,\n",
    "    CosmosDBVectorSearchType,\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "import pymongo\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229665f-185b-4da5-8722-7f2d8e048034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"example.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "COSMOS_MONGO_USER = config['cosmos_db_mongo_user']\n",
    "COSMOS_MONGO_PWD = config['cosmos_db_mongo_pwd']\n",
    "COSMOS_MONGO_SERVER = config['cosmos_db_mongo_server']\n",
    "DIMENSIONS = int(config['openai_embeddings_dimensions'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a1d64-d9e6-461c-9799-7a36132e7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"VectorSearchIndex\"\n",
    "NAMESPACE = \"ExampleDB.CachingCollection\"\n",
    "\n",
    "CONNECTION_STRING = (\"mongodb+srv://\"+COSMOS_MONGO_USER+\":\"+COSMOS_MONGO_PWD+\"@\"+COSMOS_MONGO_SERVER+\"?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\")\n",
    "\n",
    "DB_NAME, COLLECTION_NAME = NAMESPACE.split(\".\")\n",
    "mongo_client = pymongo.MongoClient(CONNECTION_STRING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228252f-641d-44c2-a6e0-8018e8308fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm_chain():\n",
    "\n",
    "    # Clear old cache if it exists\n",
    "    mongo_client[DB_NAME][COLLECTION_NAME].drop_indexes()\n",
    "    mongo_client[DB_NAME].drop_collection(COLLECTION_NAME)\n",
    "\n",
    "    # Define a template for the LLM prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You are an upbeat AI assistant who is excited to help answer questions. \n",
    "\n",
    "    Question: {question}\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \"\"\"\n",
    "    chatbot_prompt = PromptTemplate(\n",
    "        template = prompt_template, input_variables = [\"question\", \"context\"])\n",
    "\n",
    "    # Requires model version 0301 or more recent\n",
    "    # Point to completions model deployed in Azure OpenAI\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name=config['openai_completions_deployment'],\n",
    "        model_name=config['openai_completions_model'],\n",
    "        api_key=config['openai_api_key'],\n",
    "        azure_endpoint=config['openai_api_endpoint'],\n",
    "        api_version=config['openai_api_version'],\n",
    "        cache=True,\n",
    "        n=1)\n",
    "\n",
    "    # Point to embeddings model deployed in Azure OpenAI\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=config['openai_embeddings_deployment'],\n",
    "        model=config['openai_embeddings_model'],\n",
    "        api_key=config['openai_api_key'],\n",
    "        azure_endpoint=config['openai_api_endpoint'],\n",
    "        dimensions=DIMENSIONS)\n",
    "\n",
    "    # Setup simple LLM chain\n",
    "    llm_chain = LLMChain(llm = llm, prompt=chatbot_prompt)\n",
    "\n",
    "    # Setup semantic cache for LLM\n",
    "    num_lists = 1\n",
    "    similarity_algorithm = CosmosDBSimilarityType.COS\n",
    "    kind = CosmosDBVectorSearchType.VECTOR_IVF\n",
    "\n",
    "    score_threshold = 0.9\n",
    "\n",
    "    sem_cache = AzureCosmosDBSemanticCache(\n",
    "            cosmosdb_connection_string=CONNECTION_STRING,\n",
    "            cosmosdb_client=None,\n",
    "            embedding=embeddings,\n",
    "            database_name=DB_NAME,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            num_lists=num_lists,\n",
    "            similarity=similarity_algorithm,\n",
    "            kind=kind,\n",
    "            dimensions=DIMENSIONS,\n",
    "            score_threshold=score_threshold)\n",
    "\n",
    "    set_llm_cache(sem_cache)\n",
    "\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8261d88-f1e0-443f-94c3-07e4f94b867e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize llm chain\n",
    "llm_chain = init_llm_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb0417-9211-4712-ac88-3b8eceb3793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, the quesiton/response is not yet cachced in Cosmos DB, so retrieval should be slower\n",
    "llm_chain.invoke(\"Tell me something interesting about beer making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637d813-8ce1-4396-882e-64ddd729800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This quesiton/response is not yet cachced in Cosmos DB, so retrieval should be slower\n",
    "llm_chain(\"Tell me a joke about tomatoes and food.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6538a12-cf5c-4e12-aec6-b77dcc3327aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time, the quesiton/response is cached in Cosmos DB, so retrieval should be faster\n",
    "llm_chain(\"Tell me something interesting about beer making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5323b81-8a3f-4dc6-bfd1-9b7691b2b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This question is semantically similar to the previous one within the score_threshold amount, so retrieval should be faster\n",
    "llm_chain(\"How do I make beer?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
