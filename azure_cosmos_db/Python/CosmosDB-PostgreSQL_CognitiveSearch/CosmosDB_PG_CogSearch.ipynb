{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosmosDB - PostGres + Azure Cognitive Search\n",
    "This sample shows how to create and use search index on Azure Cognitive Search\n",
    "### Prerequisite: install python libraries\n",
    "- Please make sure all the libraries found in requirements.txt are installed in your python environment. \n",
    "- Rename example.env to llm.env and enter your credentials in llm.env\n",
    "- Whitelist your IP to access you CosmosDB - PostGres Cluster. Add you IP in \"Networking\" section of your PostGRes cluster on the [Azure Portal](portal.azure.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables and keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm.env\" # change to your own .env file name\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Cosmosdb Postgres (PG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "host = config[\"host\"]\n",
    "dbname = config[\"dbname\"] \n",
    "user = config[\"user\"] \n",
    "password = config[\"password\"] \n",
    "sslmode = config[\"sslmode\"] \n",
    "\n",
    "# Build a connection string from the variables\n",
    "conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20,conn_string)\n",
    "if (postgreSQL_pool):\n",
    "    print(\"Connection pool created successfully\")\n",
    "\n",
    "# Use getconn() to get a connection from the connection pool\n",
    "connection = postgreSQL_pool.getconn()\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor object to interact with the database\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute the SQL query to retrieve the table names\n",
    "cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public'\")\n",
    "\n",
    "# Fetch all the results\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "for table in table_names:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to a table in the database\n",
    "If this is the first time you are running the notebook, you need to load our sample dataset into the database first. We will create a new table \"food_reviews\" and load the data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psycopg2 import Error\n",
    "from psycopg2 import sql\n",
    "\n",
    "\n",
    "# Read data into a DataFrame\n",
    "df = pd.read_csv('../../DataSet/Reviews_small.csv')\n",
    "\n",
    "# Specify the batch size\n",
    "batch_size = 25000\n",
    "\n",
    "# Define the table name\n",
    "table_name = 'food_reviews'\n",
    "\n",
    "# # Drop previous table of same name if one exists\n",
    "# cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "# print(\"Finished dropping table (if existed)\")\n",
    "# connection.commit()\n",
    "\n",
    "# Execute the query to check if the table exists\n",
    "cursor.execute(f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = '{table_name}');\")\n",
    "\n",
    "# Fetch the result\n",
    "exists = cursor.fetchone()[0]\n",
    "\n",
    "if exists:\n",
    "    print(f\"The table '{table_name}' exists in the database.\")\n",
    "    print(\"You may drop previous table (see commented code above) if you want to re-insert reviews.\")\n",
    "else:\n",
    "    print(f\"The table '{table_name}' does not exist in the database.Creating it now and inserting reviews ...\")\n",
    "\n",
    "    # Build a connection string from the variables\n",
    "    conn_string = \"host={0} user={1} dbname={2} password={3} sslmode={4}\".format(host, user, dbname, password, sslmode)\n",
    "\n",
    "    postgreSQL_pool = psycopg2.pool.SimpleConnectionPool(1, 20, conn_string)\n",
    "    if postgreSQL_pool:\n",
    "        print(\"Connection pool created successfully\")\n",
    "\n",
    "    # Use getconn() to get a connection from the connection pool\n",
    "    with postgreSQL_pool.getconn() as connection:\n",
    "\n",
    "        # Define the CREATE TABLE query\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            Id INTEGER PRIMARY KEY,\n",
    "            ProductId TEXT,\n",
    "            UserId TEXT,\n",
    "            ProfileName TEXT,\n",
    "            HelpfulnessNumerator INTEGER,\n",
    "            HelpfulnessDenominator INTEGER,\n",
    "            Score INTEGER,\n",
    "            Time INTEGER,\n",
    "            Summary TEXT,\n",
    "            Text TEXT\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the CREATE TABLE query\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(create_table_query)\n",
    "                connection.commit()\n",
    "                print(f\"Table {table_name} created successfully!\")\n",
    "        except (Exception, Error) as e:\n",
    "            print(f\"Error creating table {table_name}: {e}\")\n",
    "            connection.rollback()\n",
    "\n",
    "        # Assuming your DataFrame variable is called 'df'\n",
    "        # Convert DataFrame to a list of tuples for bulk insertion\n",
    "        records = df.to_records(index=False)\n",
    "        records_list = records.tolist()\n",
    "\n",
    "        # Convert numpy.int32 to int in each row\n",
    "        records_list = [\n",
    "            tuple(int(value) if isinstance(value, np.int32) else value for value in record)\n",
    "            for record in records_list\n",
    "        ]\n",
    "\n",
    "        # Split the records list into batches\n",
    "        batches = [records_list[i: i + batch_size] for i in range(0, len(records_list), batch_size)]\n",
    "\n",
    "        # Iterate over each batch and perform bulk insert\n",
    "        count = 0\n",
    "        for batch in batches:\n",
    "            count += 1\n",
    "            print(f\"Inserting batch {count} into the table\")\n",
    "            try:\n",
    "                insert_query = sql.SQL(f\"INSERT INTO {table_name} (Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text) \" \\\n",
    "                                    f\"VALUES ({', '.join(['%s'] * len(batch[0]))})\")\n",
    "                \n",
    "                with connection.cursor() as cursor:\n",
    "                    cursor.executemany(insert_query, batch)\n",
    "                    connection.commit()\n",
    "            except (Exception, Error) as e:\n",
    "                print(f\"Error inserting batch into the table: {e}\")\n",
    "                connection.rollback()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "\n",
    "# Rollback the current transaction\n",
    "connection.rollback()\n",
    "cursor = connection.cursor()\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(f\"SELECT count(Id) FROM {table_name};\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except (Exception, Error) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from database and store the embedding in CogSearch \n",
    "In this step, we will retrieve the id and concatenated data of desired columns from database first. Then we will use azure open ai to get the text embedding. We will then store the text embedding in azure CogSearch for the future retrieval purposes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "\n",
    "# Rollback the current transaction\n",
    "connection.rollback()\n",
    "\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(f\"SELECT id, CONCAT('productid: ', productid, ' ', 'score: ', score, ' ', 'text: ', text) AS concat FROM {table_name};\")\n",
    "    rows = cursor.fetchall()\n",
    "except (Exception, Error) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the content and generate the embedding, store in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_type = config[\"openai_api_type\"] \n",
    "openai.api_key = config['openai_api_key']\n",
    "openai.api_base = config['openai_api_base'] \n",
    "openai.api_version = config['openai_api_version'] \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"openai_deployment_embedding\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "content_embeddings = []\n",
    "idx = []\n",
    "for row in rows:\n",
    "    idx.append(row[0])\n",
    "    content_embeddings.append(createEmbeddings(row[1]))\n",
    "\n",
    "# create a dataframe with the embeddings\n",
    "df = pd.DataFrame({'embeddings': content_embeddings}, index=idx) \n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the embeddings in Azure Cognitive Search Vector Store\n",
    "\n",
    "[AzureCogSearch](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) provides a simple interface to create a vector database, store and retrieve data using vector search. You can read more about [here](https://github.com/Azure/cognitive-search-vector-pr/tree/main) more about Vector Search.\n",
    "\n",
    "There are two steps to store data in AzureCogSearch vector database:\n",
    "- First, we create the index (or schema) of the vector database\n",
    "- Second, we add the chunked documents and their embeddings to the vector datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Azure Cognitive Search\n",
    "cogsearch_name = config[\"cogsearch_name\"] #TODO: fill in your cognitive search name\n",
    "cogsearch_index_name = config[\"cogsearch_index_name\"] #TODO: fill in your index name: must only contain lowercase, numbers, and dashes\n",
    "cogsearch_api_key = config[\"cogsearch_api_key\"] #TODO: fill in your api key with admin key\n",
    "\n",
    "EMBEDDING_LENGTH = 1536"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Index for Cog Search with fields as id,  and contentVector\n",
    "# Note the datatypes for each field below\n",
    "\n",
    "url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}?api-version=2023-07-01-Preview\"\n",
    "payload = json.dumps({\n",
    "  \"name\": cogsearch_index_name,\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"key\": True,\n",
    "      \"filterable\": True\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"contentVector\",\n",
    "      \"type\": \"Collection(Edm.Single)\",\n",
    "      \"searchable\": True,\n",
    "      \"retrievable\": True,\n",
    "      \"dimensions\": EMBEDDING_LENGTH,\n",
    "      \"vectorSearchConfiguration\": \"vectorConfig\"\n",
    "    }\n",
    "  ],\n",
    "  \"vectorSearch\": {\n",
    "    \"algorithmConfigurations\": [\n",
    "      {\n",
    "        \"name\": \"vectorConfig\",\n",
    "        \"kind\": \"hnsw\",\n",
    "        # \"hnswParameters\": {\n",
    "        #   \"m\": 4,\n",
    "        #   \"efConstruction\": 400,\n",
    "        #   \"metric\": \"cosine\"\n",
    "        # }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"semantic\": {\n",
    "    \"configurations\": [\n",
    "      {\n",
    "        \"name\": \"my-semantic-config\",\n",
    "        \"prioritizedFields\": {\n",
    "          \"prioritizedContentFields\": [\n",
    "            {\n",
    "              \"fieldName\": \"id\"\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'api-key': cogsearch_api_key\n",
    "}\n",
    "\n",
    "response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_append_payload(df):\n",
    "    \"\"\"append payload for batch insertion (note: max 1000 rows per insertion) of embeddings to Cognitive Search\"\"\"\n",
    "    value_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        value_list.append(\n",
    "            {\n",
    "            \"id\": str(index),\n",
    "            \"contentVector\": row['embeddings'],\n",
    "            \"@search.action\": \"upload\"\n",
    "            }\n",
    "        )\n",
    "    print('payload of size {}'.format(len(value_list)))\n",
    "    print('start: {}'.format(value_list[0]))\n",
    "    print('end: {}'.format(value_list[-1]))\n",
    "    payload = json.dumps({\n",
    "        \"value\": value_list\n",
    "    })\n",
    "    return payload\n",
    "\n",
    "def BatchInsertToCogSearch(df): \n",
    "    \"\"\"Batch insert of embeddings to Cognitive Search, note: column name must be 'embeddings'\"\"\"\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/index?api-version=2023-07-01-Preview\"\n",
    "    payload = batch_append_payload(df)\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    print(response.json())\n",
    "\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return \"Failure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchInsertToCogSearch(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Asks a Question \n",
    "In this step, the code will convert the user's question to an embedding and then retieve the top K document chunks based on the users' question using the cosine similirity. Please note that other similarity metrics can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userQuestion = \"Great Taffy\"\n",
    "retrieve_k = 3 # Retrieve the top 2 documents from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve k chnuks\n",
    "def retrieve_k_chunk(k, questionEmbedding):\n",
    "    # Retrieve the top K entries\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/search?api-version=2023-07-01-Preview\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "    \"vector\": {\n",
    "        \"value\": questionEmbedding,\n",
    "        \"fields\": \"contentVector\",\n",
    "        \"k\": k\n",
    "    }\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    output = json.loads(response.text)\n",
    "    print(response.status_code)\n",
    "    return output\n",
    "\n",
    "# Generate embeddings for the question and retrieve the top k document chunks\n",
    "questionEmbedding = createEmbeddings(userQuestion)\n",
    "output = retrieve_k_chunk(retrieve_k, questionEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the top k ids to retrieve the actual text from the database \n",
    "top_ids = []\n",
    "for i in range(len(output['value'])):\n",
    "    top_ids.append(int(output['value'][i]['id']))\n",
    "\n",
    "print(top_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve text from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "\n",
    "# Rollback the current transaction\n",
    "connection.rollback()\n",
    "\n",
    "format_ids = ', '.join(['%s'] * len(top_ids))\n",
    "\n",
    "sql = f\"SELECT CONCAT('productid: ', productid, ' ', 'score: ', score, ' ', 'text: ', text) AS concat FROM {table_name} WHERE id IN ({format_ids})\"\n",
    "\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(sql, top_ids)    \n",
    "    top_rows = cursor.fetchall()\n",
    "    for row in top_rows:\n",
    "        print(row)\n",
    "except (Exception, Error) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Offer Response to User's Question\n",
    "In order to offer a response, a user can either follow a simple prompting method as shown below or leverage more sophisticated ways used by other libraries, such as [langchain](https://python.langchain.com/en/latest/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting directly using Azure Open AI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    context :{context}\n",
    "    Answer the question based on the context above. Provide the product id associated with the answer as well. If the\n",
    "    information to answer the question is not present in the given context then reply \"I don't know\".\n",
    "    Question: {query}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the context from the top_rows\n",
    "context = \"\"\n",
    "for row in top_rows:\n",
    "    context += row[0]\n",
    "    context += \"\\n\"\n",
    "    \n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(userQuestion)\n",
    "prompt = template.format(context=context, query=userQuestion)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = openai.Completion.create(\n",
    "    engine= config[\"openai_deployment_completion\"],\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(\"prompt: \", prompt)\n",
    "print('~~~~~')\n",
    "# print(\"response: \", response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip())\n",
    "print(response['choices'][0]['text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
