{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cache for Redis Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample shows how to connect with an existing Redis Database with RediSearch installed, create embeddings with openAI,\n",
    "create indexes, load the vectors to the Vector Database, and query the top k results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites\n",
    "\n",
    "Set up your Redis database that we will use for a Vector Database using Azure Cache [Azure Cache with Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/quickstart-create-redis-enterprise)\n",
    "\n",
    "1. On the Advanced page, select the Modules drop-down and select RediSearch. This will create your cache with the RediSearch module installed. You can also install the other modules if youâ€™d like. \n",
    "2. In the Zone redundancy section, select Zone redundant (recommended). This will give your cache even greater availability.  \n",
    "3. In the Non-TLS access only section select Enable. \n",
    "4. For Clustering Policy, select Enterprise. RediSearch is only supported using the Enterprise cluster policy.  \n",
    "5. Select Review + create and finish creating your cache instance. \n",
    "\n",
    "- Install Python libraries using `pip install -r requirements.txt`\n",
    "- Enter your Redis environment variables for REDIS_HOST, REDIS_PORT, REDIS_PASSWORD in example.env\n",
    "- Enter your openAI environment variables in example.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "from redis import Redis\n",
    "from redis.commands.search.field import VectorField, TextField, TagField\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.result import Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the name of the .env file name \n",
    "env_name = \"example.env\"\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "redis_host = config['REDIS_HOST']\n",
    "redis_port = config['REDIS_PORT']\n",
    "redis_passwd = config['REDIS_PASSWORD']\n",
    "\n",
    "openai_api_key = config['openai_api_key']\n",
    "openai_api_base = config['openai_api_base']\n",
    "openai_api_version = config['openai_api_version']\n",
    "openai_deployment_embedding = config['openai_deployment_embedding']\n",
    "\n",
    "ITEM_KEYWORD_EMBEDDING_FIELD='item_keyword_vector'\n",
    "\n",
    "# We are using text-embedding-ada-002\n",
    "embedding_length = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_conn = Redis(\n",
    "  host=redis_host,\n",
    "  port=redis_port,\n",
    "  password=redis_passwd,\n",
    ")\n",
    "print('Connected to redis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Dataset/Reviews_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_PRODUCTS = len(df['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create content and generate embeddings using OpenAI text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will combine productid, score, and text into a single field to run embeddings on\n",
    "df['combined'] = 'productid: ' + df['ProductId'] + ' ' + 'score: ' + df['Score'].astype(str) + ' ' + 'text: ' + df['Text']\n",
    "df['combined'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = openai_api_key\n",
    "openai.api_base = openai_api_base\n",
    "openai.api_version = openai_api_version\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=openai_deployment_embedding)\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "df['embedding'] = None\n",
    "# iterate over the dataframe and create embeddings for each row\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'embedding'] = createEmbeddings(row['combined'])\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index on Id and insert our dataframe to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_index (redis_conn, vector_field_name, number_of_vectors, vector_dimensions=512, distance_metric='L2'):\n",
    "    redis_conn.ft().create_index([\n",
    "        VectorField(vector_field_name, \"FLAT\", {\"TYPE\": \"FLOAT32\", \"DIM\": vector_dimensions, \"DISTANCE_METRIC\": distance_metric, \"INITIAL_CAP\": number_of_vectors, \"BLOCK_SIZE\": number_of_vectors }),\n",
    "        TagField(\"Id\"),\n",
    "        TagField(\"ProductId\"),\n",
    "        TagField(\"UserId\"),\n",
    "        TagField(\"ProfileName\"),\n",
    "        TagField(\"HelpfulnessNumerator\"),\n",
    "        TagField(\"HelpfulnessDenominator\"),\n",
    "        TagField(\"Score\"),\n",
    "        TagField(\"Time\"),\n",
    "        TagField(\"Summary\"),\n",
    "        TextField(\"Text\"),\n",
    "        TextField(\"combined\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all data\n",
    "redis_conn.flushall()\n",
    "\n",
    "# Create flat index\n",
    "create_flat_index(redis_conn, ITEM_KEYWORD_EMBEDDING_FIELD, NUMBER_PRODUCTS, embedding_length,'COSINE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the embeddings in Redis Vector Database\n",
    "\n",
    "[Azure Cache with Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/quickstart-create-redis-enterprise) provides a simple interface to create a vector database, store and retrieve data using vector search. You can read more about Vector search [here](https://mlops.community/vector-similarity-search-from-basics-to-production/). Additionally, here is a [blog post](https://lablab.ai/t/efficient-vector-similarity-search-with-redis-a-step-by-step-tutorial) demonstrating flat index vs. HNSW. This sample uses flat index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(client:Redis, product_metadata, vector_dict, vector_field_name):\n",
    "    p = client.pipeline(transaction=False)\n",
    "    for index in product_metadata.keys():    \n",
    "        # Hash key\n",
    "        key = 'product:'+ str(index)+ ':' + product_metadata[index]['UserId']\n",
    "        \n",
    "        # Hash values\n",
    "        item_metadata = product_metadata[index]\n",
    "        item_keywords_vector = np.array(vector_dict[index]).astype(np.float32).tobytes()\n",
    "        item_metadata[vector_field_name] = item_keywords_vector\n",
    "        \n",
    "        # HSET\n",
    "        p.hset(key, mapping=item_metadata)\n",
    "            \n",
    "    p.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_metadata = df.drop('embedding', axis=1).head(NUMBER_PRODUCTS).to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vectors(redis_conn, product_metadata, df['embedding'], ITEM_KEYWORD_EMBEDDING_FIELD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userQuestion = \"Great taffy\"\n",
    "retrieve_k = 3 # Retrieve the top 3 documents from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the question and retrieve the top k document chunks\n",
    "questionEmbedding = createEmbeddings(userQuestion)\n",
    "questionEmbedding = np.array(questionEmbedding).astype(np.float32).tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the query\n",
    "q = Query(f'*=>[KNN {retrieve_k} @{ITEM_KEYWORD_EMBEDDING_FIELD} $vec_param AS vector_score]').sort_by('vector_score').paging(0, retrieve_k).return_fields(\n",
    "        'Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time',\n",
    "        'Summary', 'Text', 'combined', 'vector_score',\n",
    ").dialect(2)\n",
    "params_dict = {\"vec_param\": questionEmbedding}\n",
    "\n",
    "# Execute the query\n",
    "results = redis_conn.ft().search(q, query_params=params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve text from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieved = pd.DataFrame()\n",
    "for product in results.docs:\n",
    "    print('***************Product  found ************')\n",
    "    print(product.combined)\n",
    "    print('vector_score: ', product.vector_score)\n",
    "    \n",
    "    df_retrieved = pd.concat([df_retrieved, pd.DataFrame([product.__dict__], columns=product.__dict__.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Offer Response to User's Question\n",
    "To offer a response, one can either follow a simple prompting method as shown below or leverage ways used by other libraries, such as [langchain](https://python.langchain.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    context :{context}\n",
    "    Answer the question based on the context above. Provide the product id associated with the answer as well. If the\n",
    "    information to answer the question is not present in the given context then reply \"I don't know\".\n",
    "    Query: {query}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context for the prompt by combining the productid, score, and text of retrieved rows\n",
    "df_retrieved['combined'] = 'productid: ' + df_retrieved['ProductId'] + ' ' + 'score: ' + df_retrieved['Score'].astype(str) + ' ' + 'text: ' + df_retrieved['Text']\n",
    "context = '\\n'.join(df_retrieved['combined'])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=context, query=userQuestion)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine= config[\"openai_deployment_completion\"],\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
