{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComosDb - MongoDB + Azure Cogntive Search\n",
    "This sample shows how to create and use search index on Azure Cognitive Search when your data is in CosmosDB - MongoDB.\n",
    "\n",
    "### Requirements\n",
    "1. Install python packages in requirements.txt\n",
    "2. retrieve your CosmosDB connection string and update the value in your .env file. Your connection string can be found on [portal.azure.com](portal.azure.com)\n",
    "   1. Look for your CosmosDB - MongoDB account and navigate to \"Connection strings\" section. Copy any of the primary or secondary connection string in your .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"example.env\" # specify the name of your .env file name \n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "cosmos_connection_string = config['cosmos_connection_string']\n",
    "openai_api_key = config['openai_api_key']\n",
    "openai_api_base = config['openai_api_base']\n",
    "openai_api_version = config['openai_api_version']\n",
    "openai_deployment_embedding = config['openai_deployment_embedding']\n",
    "\n",
    "# Azure Cognitive Search\n",
    "cogsearch_name = config['cogsearch_name']\n",
    "cogsearch_api_key = config['cogsearch_api_key']\n",
    "\n",
    "# Your database and collection names\n",
    "db_name = \"reviews\"\n",
    "collection_name = \"food_reviews\"\n",
    "# Your cognitive search index name (must only contain lowercase, numbers, and dashes)\n",
    "cogsearch_index_name = \"ind_food_review_mongodb\"\n",
    "\n",
    "# We are using text-embedding-ada-002\n",
    "embedding_length = 1536"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(cosmos_connection_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the database and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database if it doesn't exist\n",
    "db = client[db_name]\n",
    "if db_name not in client.list_database_names():\n",
    "    # Create a database with 400 RU throughput that can be shared across\n",
    "    # the DB's collections\n",
    "    db.command({\"customAction\": \"CreateDatabase\", \"offerThroughput\": 400})\n",
    "    print(\"Created db '{}' with shared throughput.\\n\".format(db_name))\n",
    "else:\n",
    "    print(\"Using database: '{}'.\\n\".format(db_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection if it doesn't exist\n",
    "collection = db[collection_name]\n",
    "if collection_name not in db.list_collection_names():\n",
    "    # Creates a unsharded collection that uses the DBs shared throughput\n",
    "    db.command(\n",
    "        {\"customAction\": \"CreateCollection\", \"collection\": collection_name}\n",
    "    )\n",
    "    print(\"Created collection '{}'.\\n\".format(collection_name))\n",
    "else:\n",
    "    print(\"Using collection: '{}'.\\n\".format(collection_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client[db_name][collection_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index on Id and insert our dataframe to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../DataSet/Reviews_small.csv')\n",
    "\n",
    "if collection.count_documents({}) == 0:\n",
    "    collection.create_index('Id')\n",
    "    starting_index, batch_size = 0, 1000\n",
    "    while starting_index < len(df):\n",
    "        print(f\"Inserting documents with index {starting_index} to {starting_index + batch_size} into collection '{collection_name}'.\\n\")\n",
    "        collection.insert_many(df[starting_index:starting_index + batch_size].to_dict('records'))\n",
    "        starting_index += batch_size\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already contains documents.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create content and generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will combine productid, score, and text into a single field to run embeddings on\n",
    "df['combined'] = 'productid: ' + df['ProductId'] + ' ' + 'score: ' + df['Score'].astype(str) + ' ' + 'text: ' + df['Text']\n",
    "df['combined'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = openai_api_key\n",
    "openai.api_base = openai_api_base\n",
    "openai.api_version = openai_api_version\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=openai_deployment_embedding)\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "df['embeddings'] = None\n",
    "# iterate over the dataframe and create embeddings for each row\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'embeddings'] = createEmbeddings(row['combined'])\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the embeddings in Azure Cognitive Search Vector Store\n",
    "\n",
    "[Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) provides a simple interface to create a vector database, store and retrieve data using vector search. You can read more about Vector search [here](https://github.com/Azure/cognitive-search-vector-pr/tree/main).\n",
    "\n",
    "There are two steps to store data in AzureCogSearch vector database:\n",
    "- First, we create the index (or schema) of the vector database\n",
    "- Second, we add the chunked documents and their embeddings to the vector datastore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Index for Cog Search with fields as id,  and contentVector\n",
    "# Note the datatypes for each field below\n",
    "\n",
    "url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}?api-version=2023-07-01-Preview\"\n",
    "payload = json.dumps({\n",
    "  \"name\": cogsearch_index_name,\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"key\": True,\n",
    "      \"filterable\": True\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"contentVector\",\n",
    "      \"type\": \"Collection(Edm.Single)\",\n",
    "      \"searchable\": True,\n",
    "      \"retrievable\": True,\n",
    "      \"dimensions\": embedding_length,\n",
    "      \"vectorSearchConfiguration\": \"vectorConfig\"\n",
    "    }\n",
    "  ],\n",
    "  \"vectorSearch\": {\n",
    "    \"algorithmConfigurations\": [\n",
    "      {\n",
    "        \"name\": \"vectorConfig\",\n",
    "        \"kind\": \"hnsw\",\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"semantic\": {\n",
    "    \"configurations\": [\n",
    "      {\n",
    "        \"name\": \"my-semantic-config\",\n",
    "        \"prioritizedFields\": {\n",
    "          \"prioritizedContentFields\": [\n",
    "            {\n",
    "              \"fieldName\": \"id\"\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'api-key': cogsearch_api_key\n",
    "}\n",
    "\n",
    "response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert embeddings in search index by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_append_payload(df):\n",
    "    \"\"\"append payload for batch insertion (note: max 1000 rows per insertion) of embeddings to Cognitive Search\"\"\"\n",
    "    value_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        value_list.append(\n",
    "            {\n",
    "            \"id\": str(index),\n",
    "            \"contentVector\": row['embeddings'],\n",
    "            \"@search.action\": \"upload\"\n",
    "            }\n",
    "        )\n",
    "    print('payload of size {}'.format(len(value_list)))\n",
    "    print('start: {}'.format(value_list[0]))\n",
    "    print('end: {}'.format(value_list[-1]))\n",
    "    payload = json.dumps({\n",
    "        \"value\": value_list\n",
    "    })\n",
    "    return payload\n",
    "\n",
    "def BatchInsertToCogSearch(df):\n",
    "    \"\"\"Batch insertion of embedding to Cognitive Search, note: column name must be 'embeddings'\"\"\"\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/index?api-version=2023-07-01-Preview\"\n",
    "    payload = batch_append_payload(df)\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    print(response.json())\n",
    "\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return \"Failure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchInsertToCogSearch(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userQuestion = \"Great taffy\"\n",
    "retrieve_k = 3 # Retrieve the top 3 documents from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve k chnuks\n",
    "def retrieve_k_chunk(k, questionEmbedding):\n",
    "    # Retrieve the top K entries\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/search?api-version=2023-07-01-Preview\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "    \"vector\": {\n",
    "        \"value\": questionEmbedding,\n",
    "        \"fields\": \"contentVector\",\n",
    "        \"k\": k\n",
    "    }\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    print(response.status_code)\n",
    "    output = json.loads(response.text)\n",
    "    return output\n",
    "\n",
    "# Generate embeddings for the question and retrieve the top k document chunks\n",
    "questionEmbedding = createEmbeddings(userQuestion)\n",
    "output = retrieve_k_chunk(retrieve_k, questionEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids = [int(value['id']) for value in output['value']]\n",
    "matching_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve text from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(collection.find({'Id': {'$in': matching_ids}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retrieved = pd.DataFrame(documents)\n",
    "df_retrieved.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Offer Response to User's Question\n",
    "To offer a response, one can either follow a simple prompting method as shown below or leverage ways used by other libraries, such as [langchain](https://python.langchain.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    context :{context}\n",
    "    Answer the question based on the context above. Provide the product id associated with the answer as well. If the\n",
    "    information to answer the question is not present in the given context then reply \"I don't know\".\n",
    "    Query: {query}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context for the prompt by combining the productid, score, and text of retrieved rows\n",
    "df_retrieved['combined'] = 'productid: ' + df_retrieved['ProductId'] + ' ' + 'score: ' + df_retrieved['Score'].astype(str) + ' ' + 'text: ' + df_retrieved['Text']\n",
    "context = '\\n'.join(df_retrieved['combined'])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=context, query=userQuestion)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine= config[\"openai_deployment_completion\"],\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
